{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime2SecondsFrom1970(timeDateStr:str):\n",
    "    time1=datetime.datetime.strptime(timeDateStr,\"%Y-%m-%d %H:%M:%S\")\n",
    "    secondsFrom1970=time.mktime(time1.timetuple())\n",
    "    return secondsFrom1970\n",
    "\n",
    "def seconds2Datetime(seconds_from_1970):\n",
    "    timeArray = time.localtime(seconds_from_1970)#1970秒数\n",
    "    otherStyleTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)\n",
    "    datetime1=datetime.datetime.strptime(otherStyleTime, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(datetime1)\n",
    "def datetime2SecondsFrom1970_s(timeDateStr:str):\n",
    "    time1=datetime.datetime.strptime(timeDateStr,\"%Y%m%d%H%M%S\")\n",
    "    secondsFrom1970=time.mktime(time1.timetuple())\n",
    "    return int(secondsFrom1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_sample_test=pd.read_csv('../Data/A_preliminary_generate/Test/ad_sample_feature2phase_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traint_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_3_19=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0319_feature2phase_train.csv')\n",
    "valid_set_from_3_19=train_set_3_19[:2000]#设定验证集\n",
    "train_set_3_19=train_set_3_19[2000:]\n",
    "train_set_3_18=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0318_feature2phase_train.csv')\n",
    "train_set_3_17=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0317_feature2phase_train.csv')\n",
    "train_set_3_16=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0316_feature2phase_train.csv')\n",
    "train_set_3_15=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0315_feature2phase_train.csv')\n",
    "train_set_3_14=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0314_feature2phase_train.csv')\n",
    "train_set_3_13=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0313_feature2phase_train.csv')\n",
    "train_set_3_12=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0312_feature2phase_train.csv')\n",
    "train_set_3_11=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0311_feature2phase_train.csv')\n",
    "train_set_3_10=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0310_feature2phase_train.csv')\n",
    "train_set_3_09=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0309_feature2phase_train.csv')\n",
    "train_set_3_08=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0308_feature2phase_train.csv')\n",
    "train_set_3_07=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0307_feature2phase_train.csv')\n",
    "train_set_3_06=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0306_feature2phase_train.csv')\n",
    "train_set_3_05=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0305_feature2phase_train.csv')\n",
    "train_set_3_04=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0304_feature2phase_train.csv')\n",
    "# train_set_3_03=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0303_feature1phase_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236611 entries, 2000 to 18527\n",
      "Columns: 129 entries, ad_id to users_watch_ad_median\n",
      "dtypes: float64(53), int64(75), object(1)\n",
      "memory usage: 234.7+ MB\n",
      "train\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 129 entries, ad_id to users_watch_ad_median\n",
      "dtypes: float64(53), int64(75), object(1)\n",
      "memory usage: 2.0+ MB\n",
      "valid\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "train_set=pd.DataFrame([])\n",
    "train_set=pd.concat([train_set,train_set_3_19],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_18],axis=0)\n",
    "train_set['ad_trades_id']=train_set['ad_trades_id'].apply(lambda x: int(str(x).split(',')[0]))\n",
    "#3月18,19日两天有275条样本交易行业超过一个，但是test_sample中不存在这样的样本，直接取第一个\n",
    "train_set=pd.concat([train_set,train_set_3_17],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_16],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_15],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_14],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_13],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_12],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_11],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_10],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_09],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_08],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_07],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_06],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_05],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_04],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_03],axis=0)\n",
    "print('train\\n',train_set.info())\n",
    "print('valid\\n',valid_set_from_3_19.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236611\n",
      "236611\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228871 entries, 0 to 228870\n",
      "Columns: 130 entries, index to users_watch_ad_median\n",
      "dtypes: float64(53), int64(76), object(1)\n",
      "memory usage: 227.0+ MB\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape[0])\n",
    "train_set.drop_duplicates(inplace=True)\n",
    "print(train_set.shape[0])#去重\n",
    "train_set=train_set[(train_set['hold_time']>10800)]#持续时间需要超过三个小时的样本\n",
    "# train_set=train_set[(train_set['audience_orientation_nums']>0)]#定向人群数量为0的是有问题的\n",
    "train_set.reset_index(inplace=True)\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation_set split  &  feature fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list=[\n",
    "# 'ad_id','audience_targeting','end_time','ad_account_id','commodity_id','commodity_type','ad_trades_id','ad_size','hold_time',\n",
    "#  'create_time','sample_id', 'exposure_ad_id',\n",
    " 'bid','audience_orientation_nums','0000_0030','0030_0100','0100_0130','0130_0200','0200_0230','0230_0300','0300_0330',\n",
    "'0330_0400','0400_0430','0430_0500','0500_0530','0530_0600','0600_0630','0630_0700','0700_0730','0730_0800','0800_0830','0830_0900',\n",
    "'0900_0930','0930_1000','1000_1030','1030_1100','1100_1130','1130_1200','1200_1230','1230_1300','1300_1330','1330_1400','1400_1430','1430_1500',\n",
    "'1500_1530','1530_1600','1600_1630','1630_1700','1700_1730','1730_1800','1800_1830','1830_1900','1900_1930','1930_2000','2000_2030','2030_2100',\n",
    "'2100_2130','2130_2200','2200_2230','2230_2300','2300_2330','2330_2400',\n",
    "'ad_id_history_exposure_times','ad_id_bid_mean','ad_id_bid_var','ad_id_pctr_mean','ad_id_pctr_var','ad_id_quality_ecpm_mean',\n",
    "'ad_id_quality_ecpm_var','ad_id_total_ecpm_mean','ad_id_total_ecpm_var', \n",
    "'account_id_history_exposure_times','account_id_bid_mean','account_id_bid_var','account_id_pctr_mean','account_id_pctr_var',\n",
    "'account_id_quality_ecpm_mean','account_id_quality_ecpm_var','account_id_total_ecpm_mean','account_id_total_ecpm_var',\n",
    "'commodity_id_history_exposure_times','commodity_id_bid_mean','commodity_id_bid_var','commodity_id_pctr_mean','commodity_id_pctr_var',\n",
    "'commodity_id_quality_ecpm_mean','commodity_id_quality_ecpm_var','commodity_id_total_ecpm_mean','commodity_id_total_ecpm_var',\n",
    "'commodity_type_history_exposure_times','commodity_type_bid_mean','commodity_type_bid_var','commodity_type_pctr_mean',\n",
    "'commodity_type_pctr_var','commodity_type_quality_ecpm_mean','commodity_type_quality_ecpm_var','commodity_type_total_ecpm_mean',\n",
    "'commodity_type_total_ecpm_var',\n",
    "'trades_id_history_exposure_times','trades_id_bid_mean','trades_id_bid_var','trades_id_pctr_mean','trades_id_pctr_var',\n",
    "'trades_id_quality_ecpm_mean','trades_id_quality_ecpm_var','trades_id_total_ecpm_mean','trades_id_total_ecpm_var',\n",
    "'week_day','have_create_seconds',\n",
    "#feature phase 2\n",
    "'have_create_days', 'history_exposure_times_mean', 'age_item_nums', 'gender_item_nums',\n",
    "'area_item_nums', 'status_item_nums', 'education_item_nums', 'consuption_ability_item_nums', 'device_item_nums',\n",
    "'work_item_nums', 'connection_type_item_nums', 'behavior_item_nums', 'day_before_exposure_times', 'users_watch_ad_sum',\n",
    "'users_watch_ad_mean', 'users_watch_ad_var', 'users_watch_ad_max', 'users_watch_ad_median'\n",
    "]\n",
    "train_set.sort_values(by=['ad_id','bid'],inplace=True)\n",
    "label=train_set['exposure_times']\n",
    "train=train_set[columns_list]\n",
    "test=ad_sample_test[columns_list]\n",
    "label_validtion=valid_set_from_3_19['exposure_times']\n",
    "validtion=valid_set_from_3_19[columns_list]\n",
    "\n",
    "label_valid_array=label_validtion.values\n",
    "validtion_array=validtion.values\n",
    "train_array=train.values\n",
    "label_array=label.values\n",
    "test_array=test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    biger_lesser_equation=(preds-labels)/(abs(preds-labels)+1e-16)\n",
    "    grad = biger_lesser_equation*(4*labels/(preds+labels)**2)\n",
    "    hess = (-1*biger_lesser_equation)*(8*labels/(preds+labels)**3)\n",
    "    return 10*grad, 10*hess\n",
    "def smape_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    temp_differ_abs=abs(preds-labels)\n",
    "    temp_sum_mod_2=(preds+labels)/2\n",
    "    return 'smape',sum(temp_differ_abs/temp_sum_mod_2)/labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(valid_,label_valid_array):\n",
    "    temp_differ_abs=abs(valid_-label_valid_array)\n",
    "    temp_sum_mod_2=(valid_+label_valid_array)/2\n",
    "    \n",
    "    print(40*(1-(sum(temp_differ_abs/temp_sum_mod_2)/label_valid_array.shape[0]/2)))\n",
    "    return sum(temp_differ_abs/temp_sum_mod_2)/label_valid_array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[0]\ttrain-rmse:93.5776\tvalid_data-rmse:73.3856\ttrain-smape:1.81778\tvalid_data-smape:1.81821\n",
      "Multiple eval metrics have been passed: 'valid_data-smape' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-smape hasn't improved in 500 rounds.\n",
      "[100]\ttrain-rmse:93.1804\tvalid_data-rmse:72.8801\ttrain-smape:1.65464\tvalid_data-smape:1.65666\n",
      "[200]\ttrain-rmse:92.5442\tvalid_data-rmse:72.0756\ttrain-smape:1.58953\tvalid_data-smape:1.5929\n",
      "[300]\ttrain-rmse:91.8196\tvalid_data-rmse:71.1579\ttrain-smape:1.60054\tvalid_data-smape:1.60062\n",
      "[400]\ttrain-rmse:91.3622\tvalid_data-rmse:70.5943\ttrain-smape:1.6709\tvalid_data-smape:1.66904\n",
      "[500]\ttrain-rmse:92.3579\tvalid_data-rmse:71.9268\ttrain-smape:1.76141\tvalid_data-smape:1.75857\n",
      "[600]\ttrain-rmse:98.9929\tvalid_data-rmse:80.3121\ttrain-smape:1.83902\tvalid_data-smape:1.8359\n",
      "[700]\ttrain-rmse:111.549\tvalid_data-rmse:95.3955\ttrain-smape:1.87494\tvalid_data-smape:1.87209\n",
      "Stopping. Best iteration:\n",
      "[240]\ttrain-rmse:92.2463\tvalid_data-rmse:71.698\ttrain-smape:1.58533\tvalid_data-smape:1.5873\n",
      "\n",
      "fold n°2\n",
      "[0]\ttrain-rmse:86.4593\tvalid_data-rmse:102.532\ttrain-smape:1.81777\tvalid_data-smape:1.81843\n",
      "Multiple eval metrics have been passed: 'valid_data-smape' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-smape hasn't improved in 500 rounds.\n",
      "[100]\ttrain-rmse:86.049\tvalid_data-rmse:102.167\ttrain-smape:1.65407\tvalid_data-smape:1.65598\n",
      "[200]\ttrain-rmse:85.3279\tvalid_data-rmse:101.52\ttrain-smape:1.58861\tvalid_data-smape:1.59338\n",
      "[300]\ttrain-rmse:84.5393\tvalid_data-rmse:100.808\ttrain-smape:1.59931\tvalid_data-smape:1.60288\n",
      "[400]\ttrain-rmse:84.0507\tvalid_data-rmse:100.334\ttrain-smape:1.66968\tvalid_data-smape:1.67249\n",
      "[500]\ttrain-rmse:85.2889\tvalid_data-rmse:101.3\ttrain-smape:1.76235\tvalid_data-smape:1.76506\n",
      "[600]\ttrain-rmse:92.4409\tvalid_data-rmse:107.319\ttrain-smape:1.83795\tvalid_data-smape:1.84063\n",
      "[700]\ttrain-rmse:105.739\tvalid_data-rmse:118.908\ttrain-smape:1.87385\tvalid_data-smape:1.87614\n",
      "Stopping. Best iteration:\n",
      "[232]\ttrain-rmse:85.0544\tvalid_data-rmse:101.274\ttrain-smape:1.58426\tvalid_data-smape:1.5891\n",
      "\n",
      "fold n°3\n",
      "[0]\ttrain-rmse:89.0219\tvalid_data-rmse:93.344\ttrain-smape:1.81791\tvalid_data-smape:1.81772\n",
      "Multiple eval metrics have been passed: 'valid_data-smape' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-smape hasn't improved in 500 rounds.\n",
      "[100]\ttrain-rmse:88.5899\tvalid_data-rmse:92.9415\ttrain-smape:1.65368\tvalid_data-smape:1.65412\n",
      "[200]\ttrain-rmse:87.9488\tvalid_data-rmse:92.3327\ttrain-smape:1.58909\tvalid_data-smape:1.59289\n",
      "[300]\ttrain-rmse:87.1842\tvalid_data-rmse:91.6048\ttrain-smape:1.60037\tvalid_data-smape:1.603\n",
      "[400]\ttrain-rmse:86.6815\tvalid_data-rmse:91.1239\ttrain-smape:1.67065\tvalid_data-smape:1.67154\n",
      "[500]\ttrain-rmse:87.7676\tvalid_data-rmse:92.1508\ttrain-smape:1.76046\tvalid_data-smape:1.76205\n",
      "[600]\ttrain-rmse:94.3441\tvalid_data-rmse:98.4328\ttrain-smape:1.83695\tvalid_data-smape:1.83857\n",
      "[700]\ttrain-rmse:107.105\tvalid_data-rmse:110.723\ttrain-smape:1.87359\tvalid_data-smape:1.87491\n",
      "Stopping. Best iteration:\n",
      "[238]\ttrain-rmse:87.6441\tvalid_data-rmse:92.0426\ttrain-smape:1.58491\tvalid_data-smape:1.58902\n",
      "\n",
      "fold n°4\n",
      "[0]\ttrain-rmse:92.4987\tvalid_data-rmse:78.6696\ttrain-smape:1.81775\tvalid_data-smape:1.81825\n",
      "Multiple eval metrics have been passed: 'valid_data-smape' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-smape hasn't improved in 500 rounds.\n",
      "[100]\ttrain-rmse:92.1158\tvalid_data-rmse:78.2363\ttrain-smape:1.65451\tvalid_data-smape:1.65724\n",
      "[200]\ttrain-rmse:91.5116\tvalid_data-rmse:77.5528\ttrain-smape:1.58936\tvalid_data-smape:1.5959\n",
      "[300]\ttrain-rmse:90.7679\tvalid_data-rmse:76.7126\ttrain-smape:1.59995\tvalid_data-smape:1.60464\n",
      "[400]\ttrain-rmse:90.2952\tvalid_data-rmse:76.1985\ttrain-smape:1.6702\tvalid_data-smape:1.67287\n",
      "[500]\ttrain-rmse:91.3144\tvalid_data-rmse:77.457\ttrain-smape:1.76118\tvalid_data-smape:1.76193\n",
      "[600]\ttrain-rmse:97.9257\tvalid_data-rmse:85.2011\ttrain-smape:1.8381\tvalid_data-smape:1.83836\n",
      "[700]\ttrain-rmse:110.702\tvalid_data-rmse:99.6609\ttrain-smape:1.87448\tvalid_data-smape:1.8749\n",
      "Stopping. Best iteration:\n",
      "[240]\ttrain-rmse:91.2104\tvalid_data-rmse:77.2116\ttrain-smape:1.58502\tvalid_data-smape:1.59096\n",
      "\n",
      "fold n°5\n",
      "[0]\ttrain-rmse:87.7489\tvalid_data-rmse:98.0435\ttrain-smape:1.81823\tvalid_data-smape:1.81648\n",
      "Multiple eval metrics have been passed: 'valid_data-smape' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-smape hasn't improved in 500 rounds.\n",
      "[100]\ttrain-rmse:87.3337\tvalid_data-rmse:97.6777\ttrain-smape:1.6545\tvalid_data-smape:1.65389\n",
      "[200]\ttrain-rmse:86.6505\tvalid_data-rmse:97.0777\ttrain-smape:1.58983\tvalid_data-smape:1.59157\n",
      "[300]\ttrain-rmse:85.8337\tvalid_data-rmse:96.3607\ttrain-smape:1.6006\tvalid_data-smape:1.5997\n",
      "[400]\ttrain-rmse:85.3431\tvalid_data-rmse:95.9367\ttrain-smape:1.67124\tvalid_data-smape:1.66823\n",
      "[500]\ttrain-rmse:86.5371\tvalid_data-rmse:97.0037\ttrain-smape:1.76355\tvalid_data-smape:1.76086\n",
      "[600]\ttrain-rmse:93.7825\tvalid_data-rmse:103.518\ttrain-smape:1.83898\tvalid_data-smape:1.83702\n",
      "[700]\ttrain-rmse:107.031\tvalid_data-rmse:115.654\ttrain-smape:1.87448\tvalid_data-smape:1.87293\n",
      "Stopping. Best iteration:\n",
      "[236]\ttrain-rmse:86.329\tvalid_data-rmse:96.7963\ttrain-smape:1.58544\tvalid_data-smape:1.58647\n",
      "\n",
      "CV score: 7840.00821359\n",
      "8.22855859353217\n",
      "cv smape: 1.5885720703233914\n",
      "validation: 35005.701866012394\n",
      "8.458225868517331\n",
      "1.5770887065741335\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 10}\n",
    "# xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "#            'silent': True, 'nthread': 10}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "oof_xgb = np.zeros(len(train_array))\n",
    "predictions_xgb = np.zeros(len(test_array))\n",
    "valid_xgb=np.zeros(len(label_valid_array))\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_array, label_array)):\n",
    "    print(\"fold n°{}\".format(fold_ + 1))\n",
    "    trn_data = xgb.DMatrix(train_array[trn_idx], label_array[trn_idx])\n",
    "    val_data = xgb.DMatrix(train_array[val_idx], label_array[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "#     clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=500,\n",
    "#                     verbose_eval=100, params=xgb_params,obj=smape_obj,feval=smape_eval)\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=500,\n",
    "                    verbose_eval=100, params=xgb_params)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(train_array[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(test_array), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    valid_xgb += clf.predict(xgb.DMatrix(validtion_array), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, label_array)))\n",
    "print(\"cv smape:\",smape(oof_xgb, label_array))\n",
    "print('validation:',mean_squared_error(valid_xgb, label_valid_array))\n",
    "print(smape(valid_xgb,label_valid_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=pd.read_csv(\"../Data/A_preliminary/testA/test_sample.dat\",header=None,sep='\\t',\n",
    "                        names=['sample_id','ad_id','create_time','ad_size','ad_trades_id','commodity_type','commodity_id','ad_account_id',\n",
    "                              'when_ad_put','audience_targeting','bid'])\n",
    "print(test_sample.head())\n",
    "submission=test_sample[['sample_id','ad_id','bid']]\n",
    "submission.insert(submission.shape[1], 'predict_exposure', predictions_xgb)\n",
    "# submission.insert(submission.shape[1], 'predict_exposure', predictions_lgb)\n",
    "\n",
    "submission['predict_exposure']=submission['predict_exposure'].apply(lambda x : round(x,4))\n",
    "ad_id_exposure_predic=submission.groupby(['ad_id'])['predict_exposure'].aggregate(['mean','max','min']).reset_index()\n",
    "ad_id_dict={}\n",
    "for ad_id,mean,min_,max_ in zip(ad_id_exposure_predic['ad_id'],ad_id_exposure_predic['mean'],ad_id_exposure_predic['min'],ad_id_exposure_predic['max']):\n",
    "#     ad_id_dict[ad_id]=mean-(max_-min_)/8\n",
    "    ad_id_dict[ad_id]=mean\n",
    "#     ad_id_dict[ad_id]=max_\n",
    "\n",
    "print('how many ad_id exposure less 1 times :')\n",
    "nums=0\n",
    "for _ in ad_id_dict.values():\n",
    "    if _ < 1:\n",
    "        nums+=1\n",
    "print(nums)\n",
    "print('submission_csv mean',submission_csv['explosion'].mean())\n",
    "\n",
    "print('Monotonic adjust!!!')\n",
    "ad_id_explosure_df=pd.DataFrame({\"ad_id\":list(ad_id_dict.keys()),\"explosion_init\":list(ad_id_dict.values())})\n",
    "submission_csv=test_sample[['sample_id','ad_id','bid']][:]\n",
    "submission_csv=submission_csv.merge(ad_id_explosure_df,on='ad_id',how='outer')\n",
    "submission_csv['explosion']=submission_csv['explosion_init']+submission_csv['bid']/1000\n",
    "submission_csv['explosion']=submission_csv['explosion'].apply(lambda x:round(x,4))\n",
    "# submission_csv[['sample_id','explosion']].to_csv('submission.csv',header=None,index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
