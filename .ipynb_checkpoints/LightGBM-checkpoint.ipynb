{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime2SecondsFrom1970(timeDateStr:str):\n",
    "    time1=datetime.datetime.strptime(timeDateStr,\"%Y-%m-%d %H:%M:%S\")\n",
    "    secondsFrom1970=time.mktime(time1.timetuple())\n",
    "    return secondsFrom1970\n",
    "\n",
    "def seconds2Datetime(seconds_from_1970):\n",
    "    timeArray = time.localtime(seconds_from_1970)#1970秒数\n",
    "    otherStyleTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)\n",
    "    datetime1=datetime.datetime.strptime(otherStyleTime, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(datetime1)\n",
    "def datetime2SecondsFrom1970_s(timeDateStr:str):\n",
    "    time1=datetime.datetime.strptime(timeDateStr,\"%Y%m%d%H%M%S\")\n",
    "    secondsFrom1970=time.mktime(time1.timetuple())\n",
    "    return int(secondsFrom1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestSet load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "ad_sample_test=pd.read_csv('../Data/A_preliminary_generate/Test/ad_sample_feature1phase_test.csv')\n",
    "\n",
    "ad_sample_test['have_create_seconds']=0\n",
    "\n",
    "ad_sample_test['have_create_seconds'][\n",
    "    ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190319235959')]=datetime2SecondsFrom1970_s('20190320235959')-ad_sample_test['create_time'][ad_sample_test['create_time']\n",
    "                                                                                                                                                 <=datetime2SecondsFrom1970_s('20190319235959')]\n",
    "#预测3月20日周三\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190319235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190320235959'))]=datetime2SecondsFrom1970_s('20190321235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190319235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190320235959'))]\n",
    "#3月21日周四\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190320235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190321235959'))]=datetime2SecondsFrom1970_s('20190322235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190320235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190321235959'))]\n",
    "#3月22日周五\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190321235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190322235959'))]=datetime2SecondsFrom1970_s('20190323235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190321235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190322235959'))]\n",
    "#3月23日周六\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190322235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190323235959'))]=datetime2SecondsFrom1970_s('20190324235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190322235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190323235959'))]\n",
    "#3月24日周日\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190323235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190324235959'))]=datetime2SecondsFrom1970_s('20190325235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190323235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190324235959'))]\n",
    "#3月25日周一\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 biger than 0: (5001, 109) equal 0: (12398, 109)\n",
      "4 biger than 0: (4729, 109) equal 0: (13799, 109)\n",
      "5 biger than 0: (4987, 109) equal 0: (11861, 109)\n",
      "6 biger than 0: (4453, 109) equal 0: (11701, 109)\n",
      "7 biger than 0: (4489, 109) equal 0: (10629, 109)\n",
      "8 biger than 0: (4785, 109) equal 0: (11009, 109)\n",
      "9 biger than 0: (4780, 109) equal 0: (9562, 109)\n",
      "10 biger than 0: (4759, 109) equal 0: (10244, 109)\n",
      "11 biger than 0: (4637, 109) equal 0: (10856, 109)\n",
      "12 biger than 0: (4763, 109) equal 0: (10364, 109)\n",
      "13 biger than 0: (4837, 109) equal 0: (10949, 109)\n",
      "14 biger than 0: (5090, 109) equal 0: (11247, 109)\n",
      "15 biger than 0: (5137, 109) equal 0: (12185, 109)\n",
      "16 biger than 0: (4257, 109) equal 0: (7132, 109)\n",
      "17 biger than 0: (4346, 109) equal 0: (7560, 109)\n",
      "18 biger than 0: (3511, 109) equal 0: (6561, 109)\n",
      "19 biger than 0: (4401, 109) equal 0: (8991, 109)\n"
     ]
    }
   ],
   "source": [
    "for day in range(3,20):\n",
    "    if day <10:\n",
    "        train_set_date=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_'+'030'+str(day)+'_feature1phase_train.csv')\n",
    "    else:\n",
    "        train_set_date=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_'+'03'+str(day)+'_feature1phase_train.csv')\n",
    "    print(day,\"biger than 0:\",train_set_date[train_set_date['exposure_times']>0].shape\n",
    "          ,\"equal 0:\",train_set_date[train_set_date['exposure_times']<=0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(valid_set_from_3_19['ad_id'])&set(train_set_3_03['ad_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_3_19[:2000][train_set_3_19['exposure_times']>0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_set load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ad_static_feature=pd.read_csv(\"../Data/A_preliminary/testA/ad_static_feature.out\",header=None,sep='\\t',\n",
    "                             names=['ad_id','create_time','ad_account_id','commodity_id','commodity_type','ad_trades_id','ad_size',\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_3_19=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0319_feature1phase_train.csv')\n",
    "# valid_set_from_3_19=train_set_3_19[:2000]#设定验证集\n",
    "# train_set_3_19=train_set_3_19[2000:]\n",
    "# train_set_3_18=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0318_feature1phase_train.csv')\n",
    "# train_set_3_17=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0317_feature1phase_train.csv')\n",
    "# # train_set_3_17=train_set_3_17[train_set_3_17['exposure_times']>0]\n",
    "# train_set_3_16=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0316_feature1phase_train.csv')\n",
    "# # train_set_3_16=train_set_3_16[train_set_3_16['exposure_times']>0]\n",
    "# train_set_3_15=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0315_feature1phase_train.csv')\n",
    "# # train_set_3_15=train_set_3_15[train_set_3_15['exposure_times']>0]\n",
    "# train_set_3_14=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0314_feature1phase_train.csv')\n",
    "# # train_set_3_14=train_set_3_14[train_set_3_14['exposure_times']>0]\n",
    "# train_set_3_13=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0313_feature1phase_train.csv')\n",
    "# # train_set_3_13=train_set_3_13[train_set_3_13['exposure_times']>0]\n",
    "# train_set_3_12=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0312_feature1phase_train.csv')\n",
    "# # train_set_3_12=train_set_3_12[train_set_3_12['exposure_times']>0]\n",
    "# train_set_3_11=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0311_feature1phase_train.csv')\n",
    "# # train_set_3_11=train_set_3_11[train_set_3_11['exposure_times']>0]\n",
    "# train_set_3_10=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0310_feature1phase_train.csv')\n",
    "# # train_set_3_10=train_set_3_10[train_set_3_10['exposure_times']>0]\n",
    "# train_set_3_09=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0309_feature1phase_train.csv')\n",
    "# # train_set_3_09=train_set_3_09[train_set_3_09['exposure_times']>0]\n",
    "# train_set_3_08=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0308_feature1phase_train.csv')\n",
    "# train_set_3_07=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0307_feature1phase_train.csv')\n",
    "# train_set_3_06=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0306_feature1phase_train.csv')\n",
    "# train_set_3_05=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0305_feature1phase_train.csv')\n",
    "# train_set_3_04=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0304_feature1phase_train.csv')\n",
    "# train_set_3_03=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0303_feature1phase_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236611 entries, 2000 to 18527\n",
      "Columns: 108 entries, ad_id to week_day\n",
      "dtypes: float64(46), int64(61), object(1)\n",
      "memory usage: 196.8+ MB\n",
      "train\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236611 entries, 0 to 236610\n",
      "Columns: 110 entries, ad_id to have_create_seconds\n",
      "dtypes: float64(46), int64(63), object(1)\n",
      "memory usage: 200.4+ MB\n",
      "train\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 0 to 1999\n",
      "Columns: 110 entries, ad_id to create_time\n",
      "dtypes: float64(46), int64(63), object(1)\n",
      "memory usage: 1.7+ MB\n",
      "valid\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "train_set_3_19=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0319_feature1phase_train.csv')\n",
    "valid_set_from_3_19=train_set_3_19[:2000]#设定验证集\n",
    "train_set_3_19=train_set_3_19[2000:]\n",
    "train_set_3_18=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0318_feature1phase_train.csv')\n",
    "train_set=pd.DataFrame([])\n",
    "train_set=pd.concat([train_set,train_set_3_19],axis=0)\n",
    "train_set=pd.concat([train_set,train_set_3_18],axis=0)\n",
    "train_set['ad_trades_id']=train_set['ad_trades_id'].apply(lambda x: int(str(x).split(',')[0]))\n",
    "#3月18,19日两天有275条样本交易行业超过一个，但是test_sample中不存在这样的样本，直接取第一个\n",
    "for day in ['0317','0316','0315','0314','0313','0312','0311','0310','0309','0308','0307','0306','0305','0304']:\n",
    "    train_set_date=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_'+day+'_feature1phase_train.csv')\n",
    "    train_set=pd.concat([train_set,train_set_date],axis=0)\n",
    "need_round_list=['ad_id_bid_mean','ad_id_bid_var','ad_id_pctr_mean','ad_id_pctr_var','ad_id_quality_ecpm_mean',\n",
    "'ad_id_quality_ecpm_var','ad_id_total_ecpm_mean','ad_id_total_ecpm_var', \n",
    "'account_id_bid_mean','account_id_bid_var','account_id_pctr_mean','account_id_pctr_var',\n",
    "'account_id_quality_ecpm_mean','account_id_quality_ecpm_var','account_id_total_ecpm_mean','account_id_total_ecpm_var',\n",
    "'commodity_id_bid_mean','commodity_id_bid_var','commodity_id_pctr_mean','commodity_id_pctr_var',\n",
    "'commodity_id_quality_ecpm_mean','commodity_id_quality_ecpm_var','commodity_id_total_ecpm_mean','commodity_id_total_ecpm_var',\n",
    "'commodity_type_bid_mean','commodity_type_bid_var','commodity_type_pctr_mean',\n",
    "'commodity_type_pctr_var','commodity_type_quality_ecpm_mean','commodity_type_quality_ecpm_var','commodity_type_total_ecpm_mean',\n",
    "'commodity_type_total_ecpm_var','trades_id_bid_mean','trades_id_bid_var','trades_id_pctr_mean','trades_id_pctr_var',\n",
    "'trades_id_quality_ecpm_mean','trades_id_quality_ecpm_var','trades_id_total_ecpm_mean','trades_id_total_ecpm_var',]\n",
    "for col in need_round_list:\n",
    "    train_set[col]=train_set[col].apply(lambda x:round(x,2))\n",
    "# train_set=pd.concat([train_set,train_set_3_17],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_16],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_15],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_14],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_13],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_12],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_11],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_10],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_09],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_08],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_07],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_06],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_05],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_04],axis=0)\n",
    "# train_set=pd.concat([train_set,train_set_3_03],axis=0)\n",
    "train_set.drop(columns='create_time',inplace=True)\n",
    "valid_set_from_3_19.drop(columns='create_time',inplace=True)\n",
    "print('train\\n',train_set.info())\n",
    "train_set=train_set.merge(ad_static_feature[['ad_id','create_time']],on='ad_id',how='inner')\n",
    "valid_set_from_3_19=valid_set_from_3_19.merge(ad_static_feature[['ad_id','create_time']],on='ad_id',how='inner')\n",
    "train_set['have_create_seconds']=train_set['end_time']-train_set['create_time']\n",
    "valid_set_from_3_19['have_create_seconds']=valid_set_from_3_19['end_time']-valid_set_from_3_19['create_time']\n",
    "print('train\\n',train_set.info())\n",
    "print('valid\\n',valid_set_from_3_19.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236611\n",
      "236611\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228871 entries, 0 to 228870\n",
      "Columns: 111 entries, index to have_create_seconds\n",
      "dtypes: float64(46), int64(64), object(1)\n",
      "memory usage: 193.8+ MB\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape[0])\n",
    "train_set.drop_duplicates(inplace=True)\n",
    "print(train_set.shape[0])#去重\n",
    "train_set=train_set[(train_set['hold_time']>10800)]#持续时间需要超过三个小时的样本\n",
    "# train_set=train_set[(train_set['audience_orientation_nums']>0)]#定向人群数量为0的是有问题的\n",
    "train_set.reset_index(inplace=True)\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation_set split  &  feature fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list=[\n",
    "# 'ad_id','audience_targeting','end_time','ad_account_id','commodity_id','commodity_type','ad_trades_id','ad_size','hold_time',\n",
    "#  'create_time','sample_id', 'exposure_ad_id',\n",
    " 'bid','audience_orientation_nums','0000_0030','0030_0100','0100_0130','0130_0200','0200_0230','0230_0300','0300_0330',\n",
    "'0330_0400','0400_0430','0430_0500','0500_0530','0530_0600','0600_0630','0630_0700','0700_0730','0730_0800','0800_0830','0830_0900',\n",
    "'0900_0930','0930_1000','1000_1030','1030_1100','1100_1130','1130_1200','1200_1230','1230_1300','1300_1330','1330_1400','1400_1430','1430_1500',\n",
    "'1500_1530','1530_1600','1600_1630','1630_1700','1700_1730','1730_1800','1800_1830','1830_1900','1900_1930','1930_2000','2000_2030','2030_2100',\n",
    "'2100_2130','2130_2200','2200_2230','2230_2300','2300_2330','2330_2400',\n",
    "'ad_id_history_exposure_times','ad_id_bid_mean','ad_id_bid_var','ad_id_pctr_mean','ad_id_pctr_var','ad_id_quality_ecpm_mean',\n",
    "'ad_id_quality_ecpm_var','ad_id_total_ecpm_mean','ad_id_total_ecpm_var', \n",
    "'account_id_history_exposure_times','account_id_bid_mean','account_id_bid_var','account_id_pctr_mean','account_id_pctr_var',\n",
    "'account_id_quality_ecpm_mean','account_id_quality_ecpm_var','account_id_total_ecpm_mean','account_id_total_ecpm_var',\n",
    "'commodity_id_history_exposure_times','commodity_id_bid_mean','commodity_id_bid_var','commodity_id_pctr_mean','commodity_id_pctr_var',\n",
    "'commodity_id_quality_ecpm_mean','commodity_id_quality_ecpm_var','commodity_id_total_ecpm_mean','commodity_id_total_ecpm_var',\n",
    "'commodity_type_history_exposure_times','commodity_type_bid_mean','commodity_type_bid_var','commodity_type_pctr_mean',\n",
    "'commodity_type_pctr_var','commodity_type_quality_ecpm_mean','commodity_type_quality_ecpm_var','commodity_type_total_ecpm_mean',\n",
    "'commodity_type_total_ecpm_var',\n",
    "'trades_id_history_exposure_times','trades_id_bid_mean','trades_id_bid_var','trades_id_pctr_mean','trades_id_pctr_var',\n",
    "'trades_id_quality_ecpm_mean','trades_id_quality_ecpm_var','trades_id_total_ecpm_mean','trades_id_total_ecpm_var',\n",
    "'week_day','have_create_seconds']\n",
    "\n",
    "label=train_set['exposure_times']\n",
    "train=train_set[columns_list]\n",
    "test=ad_sample_test[columns_list]\n",
    "label_validtion=valid_set_from_3_19['exposure_times']\n",
    "validtion=valid_set_from_3_19[columns_list]\n",
    "\n",
    "label_valid_array=label_validtion.values\n",
    "validtion_array=validtion.values\n",
    "train_array=train.values\n",
    "label_array=label.values\n",
    "test_array=test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtrain = xgb.DMatrix('agaricus.txt.train')\n",
    "def smape_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    biger_lesser_equation=(preds-labels)/(abs(preds-labels)+1e-16)\n",
    "    grad = biger_lesser_equation*(4*labels/(preds+labels)**2)\n",
    "    hess = (-1*biger_lesser_equation)*(8*labels/(preds+labels)**3)\n",
    "    return grad, hess\n",
    "def smape_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    temp_differ_abs=abs(preds-labels)\n",
    "    temp_sum_mod_2=(preds+labels)/2\n",
    "    return 'smape',sum(temp_differ_abs/temp_sum_mod_2)/labels.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(valid_,label_valid_array):\n",
    "    temp_differ_abs=abs(valid_-label_valid_array)\n",
    "    temp_sum_mod_2=(valid_+label_valid_array)/2\n",
    "    \n",
    "    print(40*(1-(sum(temp_differ_abs/temp_sum_mod_2)/label_valid_array.shape[0]/2)))\n",
    "    return sum(temp_differ_abs/temp_sum_mod_2)/label_valid_array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[0]\ttrain-rmse:92.6651\tvalid_data-rmse:76.2334\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f4cef7885d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n\u001b[0;32m---> 14\u001b[0;31m                     verbose_eval=100, params=xgb_params)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moof_xgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ntree_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions_xgb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ntree_limit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 10}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb = np.zeros(len(train_array))\n",
    "predictions_xgb = np.zeros(len(test_array))\n",
    "valid_xgb=np.zeros(len(label_valid_array))\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_array, label_array)):\n",
    "    print(\"fold n°{}\".format(fold_ + 1))\n",
    "    trn_data = xgb.DMatrix(train_array[trn_idx], label_array[trn_idx])\n",
    "    val_data = xgb.DMatrix(train_array[val_idx], label_array[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n",
    "                    verbose_eval=100, params=xgb_params)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(train_array[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(test_array), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    valid_xgb += clf.predict(xgb.DMatrix(validtion_array), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, label_array)))\n",
    "print(\"cv smape:\",smape(oof_xgb, label_array))\n",
    "print('validation:',mean_squared_error(valid_xgb, label_valid_array))\n",
    "print(smape(valid_xgb,label_valid_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission_csv generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id   ad_id  create_time  ad_size  ad_trades_id  commodity_type  \\\n",
      "0          1  394352   1529648412       34            84              13   \n",
      "1          2  585401   1553076190       40           221               1   \n",
      "2          3  419408   1553031394       30           122              13   \n",
      "3          4  405326   1553238836       64           136               1   \n",
      "4          5  578942   1541191585       34            12              13   \n",
      "\n",
      "   commodity_id  ad_account_id  \\\n",
      "0         29663          26657   \n",
      "1            -1           6262   \n",
      "2         32110          17436   \n",
      "3            -1          22359   \n",
      "4          6372          24082   \n",
      "\n",
      "                                         when_ad_put  \\\n",
      "0  281474976645120,281474976645120,28147497664512...   \n",
      "1  281474976579587,281474976579587,28147497657958...   \n",
      "2  17592185782272,17592185782272,17592185782272,1...   \n",
      "3  281474976694272,281474976694272,28147497669427...   \n",
      "4  68719214592,68719214592,68719214592,6871921459...   \n",
      "\n",
      "                                  audience_targeting  bid  \n",
      "0  age:819,608,988,741,202,837,400,394,942,361,72...  120  \n",
      "1  age:819,433,479,741,229,347,522,79,753,601|edu...   42  \n",
      "2                                                all    6  \n",
      "3  age:333,1|gender:2|area:11505,1874,3790,4566,5...  181  \n",
      "4  age:819,608,988,741,202,837,400,394,942,361,72...   31  \n",
      "how many ad_id exposure less 1 times :\n",
      "926\n",
      "Monotonic !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test_sample=pd.read_csv(\"../Data/A_preliminary/testA/test_sample.dat\",header=None,sep='\\t',\n",
    "                        names=['sample_id','ad_id','create_time','ad_size','ad_trades_id','commodity_type','commodity_id','ad_account_id',\n",
    "                              'when_ad_put','audience_targeting','bid'])\n",
    "print(test_sample.head())\n",
    "submission=test_sample[['sample_id','ad_id','bid']]\n",
    "submission.insert(submission.shape[1], 'predict_exposure', predictions_xgb)\n",
    "# submission.insert(submission.shape[1], 'predict_exposure', predictions_lgb)\n",
    "\n",
    "submission['predict_exposure']=submission['predict_exposure'].apply(lambda x : round(x,4))\n",
    "ad_id_exposure_predic=submission.groupby(['ad_id'])['predict_exposure'].aggregate(['mean','max','min']).reset_index()\n",
    "ad_id_dict={}\n",
    "for ad_id,mean,min_,max_ in zip(ad_id_exposure_predic['ad_id'],ad_id_exposure_predic['mean'],ad_id_exposure_predic['min'],ad_id_exposure_predic['max']):\n",
    "#     ad_id_dict[ad_id]=mean-(max_-min_)/8\n",
    "    ad_id_dict[ad_id]=mean\n",
    "#     ad_id_dict[ad_id]=max_\n",
    "\n",
    "print('how many ad_id exposure less 1 times :')\n",
    "nums=0\n",
    "for _ in ad_id_dict.values():\n",
    "    if _ < 1:\n",
    "        nums+=1\n",
    "print(nums)\n",
    "\n",
    "print('Monotonic !!!')\n",
    "ad_id_explosure_df=pd.DataFrame({\"ad_id\":list(ad_id_dict.keys()),\"explosion_init\":list(ad_id_dict.values())})\n",
    "submission_csv=test_sample[['sample_id','ad_id','bid']][:]\n",
    "submission_csv=submission_csv.merge(ad_id_explosure_df,on='ad_id',how='outer')\n",
    "submission_csv['explosion']=submission_csv['explosion_init']+submission_csv['bid']/1000\n",
    "submission_csv['explosion']=submission_csv['explosion'].apply(lambda x:round(x,4))\n",
    "# submission_csv[['sample_id','explosion']].to_csv('submission.csv',header=None,index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.71878838836866"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_csv['explosion'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv[['sample_id','explosion']].to_csv('submission_3_03_15_98.csv',header=None,index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smapeobj(preds, labels):\n",
    "    biger_lesser_equation=(preds-labels)/(abs(preds-labels)+1e-16)\n",
    "    grad = biger_lesser_equation*(4*labels/(preds+labels)**2)\n",
    "    hess = (-1*biger_lesser_equation)*(8*labels/(preds+labels)**3)\n",
    "    return grad, hess\n",
    "def smape(preds,labels):\n",
    "    temp_differ_abs=abs(preds-labels)\n",
    "    temp_sum_mod_2=(preds+labels)/2\n",
    "    return temp_differ_abs/temp_sum_mod_2\n",
    "pred_array=np.array([0.1,0.3,0.6,0.7,0.8,1.5,1.6,1.8,1.9,2,2.1, 2.3, 2.6, 2.7, 2.8, 3.5, 3.6, 3.8, 3.9, 4])\n",
    "label_array=np.array([2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2])\n",
    "print(smapeobj(pred_array,label_array))\n",
    "print(smape(pred_array,label_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 45.0701\tvalid_1's rmse: 112.622\n",
      "[400]\ttraining's rmse: 39.6025\tvalid_1's rmse: 107.064\n",
      "[600]\ttraining's rmse: 36.2563\tvalid_1's rmse: 104.18\n",
      "[800]\ttraining's rmse: 33.7299\tvalid_1's rmse: 102.389\n",
      "[1000]\ttraining's rmse: 31.6861\tvalid_1's rmse: 100.994\n",
      "[1200]\ttraining's rmse: 30.0162\tvalid_1's rmse: 99.8082\n",
      "[1400]\ttraining's rmse: 28.5247\tvalid_1's rmse: 98.8732\n",
      "[1600]\ttraining's rmse: 27.23\tvalid_1's rmse: 98.0081\n",
      "[1800]\ttraining's rmse: 26.1293\tvalid_1's rmse: 97.4583\n",
      "[2000]\ttraining's rmse: 25.1362\tvalid_1's rmse: 97.0622\n",
      "[2200]\ttraining's rmse: 24.2459\tvalid_1's rmse: 96.7088\n",
      "[2400]\ttraining's rmse: 23.4227\tvalid_1's rmse: 96.4222\n",
      "[2600]\ttraining's rmse: 22.6961\tvalid_1's rmse: 96.2949\n",
      "[2800]\ttraining's rmse: 22.0008\tvalid_1's rmse: 96.2284\n",
      "[3000]\ttraining's rmse: 21.3848\tvalid_1's rmse: 96.1073\n",
      "[3200]\ttraining's rmse: 20.8178\tvalid_1's rmse: 96.1185\n",
      "Early stopping, best iteration is:\n",
      "[3068]\ttraining's rmse: 21.1932\tvalid_1's rmse: 96.0566\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 61.7209\tvalid_1's rmse: 58.1156\n",
      "[400]\ttraining's rmse: 54.1265\tvalid_1's rmse: 57.0292\n",
      "[600]\ttraining's rmse: 49.2841\tvalid_1's rmse: 56.7142\n",
      "[800]\ttraining's rmse: 45.652\tvalid_1's rmse: 56.3005\n",
      "[1000]\ttraining's rmse: 42.8964\tvalid_1's rmse: 56.3087\n",
      "[1200]\ttraining's rmse: 40.6269\tvalid_1's rmse: 56.1619\n",
      "[1400]\ttraining's rmse: 38.7742\tvalid_1's rmse: 56.0975\n",
      "[1600]\ttraining's rmse: 37.1468\tvalid_1's rmse: 56.1244\n",
      "Early stopping, best iteration is:\n",
      "[1471]\ttraining's rmse: 38.1847\tvalid_1's rmse: 56.0548\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 63.4526\tvalid_1's rmse: 50.7745\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's rmse: 64.6076\tvalid_1's rmse: 50.6285\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 64.3498\tvalid_1's rmse: 39.1248\n",
      "[400]\ttraining's rmse: 56.8549\tvalid_1's rmse: 34.967\n",
      "[600]\ttraining's rmse: 51.9685\tvalid_1's rmse: 32.4035\n",
      "[800]\ttraining's rmse: 48.1809\tvalid_1's rmse: 30.9735\n",
      "[1000]\ttraining's rmse: 45.1849\tvalid_1's rmse: 30.3772\n",
      "[1200]\ttraining's rmse: 42.7813\tvalid_1's rmse: 30.158\n",
      "[1400]\ttraining's rmse: 40.7493\tvalid_1's rmse: 30.2855\n",
      "Early stopping, best iteration is:\n",
      "[1203]\ttraining's rmse: 42.7509\tvalid_1's rmse: 30.1519\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 64.2814\tvalid_1's rmse: 42.6574\n",
      "[400]\ttraining's rmse: 55.9716\tvalid_1's rmse: 39.2808\n",
      "[600]\ttraining's rmse: 50.682\tvalid_1's rmse: 38.7341\n",
      "[800]\ttraining's rmse: 46.7973\tvalid_1's rmse: 38.9182\n",
      "Early stopping, best iteration is:\n",
      "[624]\ttraining's rmse: 50.1385\tvalid_1's rmse: 38.621\n",
      "CV score: 3466.61991943\n",
      "validation: 19126.692065882457\n",
      "7.079067205458727\n",
      "1.6460466397270637\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 120,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_lgb = np.zeros(train_array.shape[0])\n",
    "predictions_lgb = np.zeros(test_array.shape[0])\n",
    "valid_lgb=np.zeros(len(label_valid_array))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_array, label_array)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(train_array[trn_idx], label_array[trn_idx])\n",
    "    val_data = lgb.Dataset(train_array[val_idx], label_array[val_idx])\n",
    "\n",
    "    num_round = 10000*(fold_+1)\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 200)\n",
    "    oof_lgb[val_idx] = clf.predict(train_array[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test_array, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    valid_lgb += clf.predict(validtion_array, ntree_limit=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, label_array)))\n",
    "print(\"cv smape:\",smape(oof_lgb, label_array))\n",
    "print('validation:',mean_squared_error(valid_lgb, label_valid_array))\n",
    "print(smape(valid_lgb,label_valid_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=pd.read_csv(\"../Data/A_preliminary/testA/test_sample.dat\",header=None,sep='\\t',\n",
    "                        names=['sample_id','ad_id','create_time','ad_size','ad_trades_id','commodity_type','commodity_id','ad_account_id',\n",
    "                              'when_ad_put','audience_targeting','bid'])\n",
    "print(test_sample.head())\n",
    "submission=test_sample[['sample_id','ad_id','bid']]\n",
    "# submission.insert(submission.shape[1], 'predict_exposure', predictions_xgb)\n",
    "submission.insert(submission.shape[1], 'predict_exposure', predictions_lgb)\n",
    "\n",
    "submission['predict_exposure']=submission['predict_exposure'].apply(lambda x : round(x,4))\n",
    "ad_id_exposure_predic=submission.groupby(['ad_id'])['predict_exposure'].aggregate(['mean','max','min']).reset_index()\n",
    "ad_id_dict={}\n",
    "for ad_id,mean,min_,max_ in zip(ad_id_exposure_predic['ad_id'],ad_id_exposure_predic['mean'],ad_id_exposure_predic['min'],ad_id_exposure_predic['max']):\n",
    "#     ad_id_dict[ad_id]=mean-(max_-min_)/8\n",
    "    ad_id_dict[ad_id]=mean\n",
    "#     ad_id_dict[ad_id]=max_\n",
    "\n",
    "\n",
    "print('how many ad_id exposure less 1 times :')\n",
    "nums=0\n",
    "for _ in ad_id_dict.values():\n",
    "    if _ < 1:\n",
    "        nums+=1\n",
    "print(nums)\n",
    "\n",
    "print('Monotonic !!!')\n",
    "ad_id_explosure_df=pd.DataFrame({\"ad_id\":list(ad_id_dict.keys()),\"explosion_init\":list(ad_id_dict.values())})\n",
    "submission_csv=test_sample[['sample_id','ad_id','bid']][:]\n",
    "submission_csv=submission_csv.merge(ad_id_explosure_df,on='ad_id',how='outer')\n",
    "submission_csv['explosion']=submission_csv['explosion_init']+submission_csv['bid']/1000\n",
    "submission_csv['explosion']=submission_csv['explosion'].apply(lambda x:round(x,4))\n",
    "# submission_csv[['sample_id','explosion']].to_csv('submission.csv',header=None,index=None,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
