{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime2SecondsFrom1970(timeDateStr:str):\n",
    "    time1=datetime.datetime.strptime(timeDateStr,\"%Y-%m-%d %H:%M:%S\")\n",
    "    secondsFrom1970=time.mktime(time1.timetuple())\n",
    "    return secondsFrom1970\n",
    "\n",
    "def seconds2Datetime(seconds_from_1970):\n",
    "    timeArray = time.localtime(seconds_from_1970)#1970秒数\n",
    "    otherStyleTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)\n",
    "    datetime1=datetime.datetime.strptime(otherStyleTime, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(datetime1)\n",
    "def datetime2SecondsFrom1970_s(timeDateStr:str):\n",
    "    time1=datetime.datetime.strptime(timeDateStr,\"%Y%m%d%H%M%S\")\n",
    "    secondsFrom1970=time.mktime(time1.timetuple())\n",
    "    return int(secondsFrom1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestSet load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "ad_sample_test=pd.read_csv('../Data/A_preliminary_generate/Test/ad_sample_feature1phase_test.csv')\n",
    "\n",
    "ad_sample_test['have_create_seconds']=0\n",
    "\n",
    "ad_sample_test['have_create_seconds'][\n",
    "    ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190319235959')]=datetime2SecondsFrom1970_s('20190320235959')-ad_sample_test['create_time'][ad_sample_test['create_time']\n",
    "                                                                                                                                                 <=datetime2SecondsFrom1970_s('20190319235959')]\n",
    "#预测3月20日周三\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190319235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190320235959'))]=datetime2SecondsFrom1970_s('20190321235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190319235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190320235959'))]\n",
    "#3月21日周四\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190320235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190321235959'))]=datetime2SecondsFrom1970_s('20190322235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190320235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190321235959'))]\n",
    "#3月22日周五\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190321235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190322235959'))]=datetime2SecondsFrom1970_s('20190323235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190321235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190322235959'))]\n",
    "#3月23日周六\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190322235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190323235959'))]=datetime2SecondsFrom1970_s('20190324235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190322235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190323235959'))]\n",
    "#3月24日周日\n",
    "\n",
    "ad_sample_test['have_create_seconds'][(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190323235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190324235959'))]=datetime2SecondsFrom1970_s('20190325235959')-ad_sample_test['create_time'][\n",
    "(ad_sample_test['create_time']>datetime2SecondsFrom1970_s('20190323235959'))&\n",
    "(ad_sample_test['create_time']<=datetime2SecondsFrom1970_s('20190324235959'))]\n",
    "#3月25日周一\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in range(3,20):\n",
    "    if day <10:\n",
    "        train_set_date=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_'+'030'+str(day)+'_feature1phase_train.csv')\n",
    "    else:\n",
    "        train_set_date=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_'+'03'+str(day)+'_feature1phase_train.csv')\n",
    "    print(day,\"biger than 0:\",train_set_date[train_set_date['exposure_times']>0].shape\n",
    "          ,\"equal 0:\",train_set_date[train_set_date['exposure_times']<=0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_set load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ad_static_feature=pd.read_csv(\"../Data/A_preliminary/testA/ad_static_feature.out\",header=None,sep='\\t',\n",
    "                             names=['ad_id','create_time','ad_account_id','commodity_id','commodity_type','ad_trades_id','ad_size',\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236611 entries, 2000 to 18527\n",
      "Columns: 108 entries, ad_id to week_day\n",
      "dtypes: float64(46), int64(61), object(1)\n",
      "memory usage: 196.8+ MB\n",
      "train\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236611 entries, 0 to 236610\n",
      "Columns: 110 entries, ad_id to have_create_seconds\n",
      "dtypes: float64(46), int64(63), object(1)\n",
      "memory usage: 200.4+ MB\n",
      "train\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 0 to 1999\n",
      "Columns: 110 entries, ad_id to have_create_seconds\n",
      "dtypes: float64(46), int64(63), object(1)\n",
      "memory usage: 1.7+ MB\n",
      "valid\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "train_set_3_19=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0319_feature1phase_train.csv')\n",
    "valid_set_from_3_19=train_set_3_19[:2000]#设定验证集\n",
    "train_set_3_19=train_set_3_19[2000:]\n",
    "train_set_3_18=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_0318_feature1phase_train.csv')\n",
    "train_set=pd.concat([train_set_3_19,train_set_3_18],axis=0)\n",
    "train_set['ad_trades_id']=train_set['ad_trades_id'].apply(lambda x: int(str(x).split(',')[0]))\n",
    "#3月18,19日两天有275条样本交易行业超过一个，但是test_sample中不存在这样的样本，直接取第一个\n",
    "for day in ['0317','0316','0315','0314','0313','0312','0311','0310','0309','0308','0307','0306','0305','0304']:\n",
    "    train_set_date=pd.read_csv('../Data/A_preliminary_generate/Train/ad_sample_'+day+'_feature1phase_train.csv')\n",
    "    train_set=pd.concat([train_set,train_set_date],axis=0)\n",
    "need_round_list=['ad_id_bid_mean','ad_id_bid_var','ad_id_pctr_mean','ad_id_pctr_var','ad_id_quality_ecpm_mean',\n",
    "'ad_id_quality_ecpm_var','ad_id_total_ecpm_mean','ad_id_total_ecpm_var', \n",
    "'account_id_bid_mean','account_id_bid_var','account_id_pctr_mean','account_id_pctr_var',\n",
    "'account_id_quality_ecpm_mean','account_id_quality_ecpm_var','account_id_total_ecpm_mean','account_id_total_ecpm_var',\n",
    "'commodity_id_bid_mean','commodity_id_bid_var','commodity_id_pctr_mean','commodity_id_pctr_var',\n",
    "'commodity_id_quality_ecpm_mean','commodity_id_quality_ecpm_var','commodity_id_total_ecpm_mean','commodity_id_total_ecpm_var',\n",
    "'commodity_type_bid_mean','commodity_type_bid_var','commodity_type_pctr_mean',\n",
    "'commodity_type_pctr_var','commodity_type_quality_ecpm_mean','commodity_type_quality_ecpm_var','commodity_type_total_ecpm_mean',\n",
    "'commodity_type_total_ecpm_var','trades_id_bid_mean','trades_id_bid_var','trades_id_pctr_mean','trades_id_pctr_var',\n",
    "'trades_id_quality_ecpm_mean','trades_id_quality_ecpm_var','trades_id_total_ecpm_mean','trades_id_total_ecpm_var',]\n",
    "for col in need_round_list:\n",
    "    train_set[col]=train_set[col].apply(lambda x:round(x,2))\n",
    "train_set.drop(columns='create_time',inplace=True)\n",
    "valid_set_from_3_19.drop(columns='create_time',inplace=True)\n",
    "print('train\\n',train_set.info())\n",
    "train_set=train_set.merge(ad_static_feature[['ad_id','create_time']],on='ad_id',how='inner')\n",
    "valid_set_from_3_19=valid_set_from_3_19.merge(ad_static_feature[['ad_id','create_time']],on='ad_id',how='inner')\n",
    "train_set['have_create_seconds']=train_set['end_time']-train_set['create_time']\n",
    "valid_set_from_3_19['have_create_seconds']=valid_set_from_3_19['end_time']-valid_set_from_3_19['create_time']\n",
    "print('train\\n',train_set.info())\n",
    "print('valid\\n',valid_set_from_3_19.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236611\n",
      "236611\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228871 entries, 0 to 228870\n",
      "Columns: 111 entries, index to have_create_seconds\n",
      "dtypes: float64(46), int64(64), object(1)\n",
      "memory usage: 193.8+ MB\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape[0])\n",
    "train_set.drop_duplicates(inplace=True)\n",
    "print(train_set.shape[0])#去重\n",
    "train_set=train_set[(train_set['hold_time']>10800)]#持续时间需要超过三个小时的样本\n",
    "# train_set=train_set[(train_set['audience_orientation_nums']>0)]#定向人群数量为0的是有问题的\n",
    "train_set.reset_index(inplace=True)\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation_set split  &  feature fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list=[\n",
    "# 'ad_id','audience_targeting','end_time','ad_account_id','commodity_id','commodity_type','ad_trades_id','ad_size','hold_time',\n",
    "#  'create_time','sample_id', 'exposure_ad_id',\n",
    " 'bid','audience_orientation_nums','0000_0030','0030_0100','0100_0130','0130_0200','0200_0230','0230_0300','0300_0330',\n",
    "'0330_0400','0400_0430','0430_0500','0500_0530','0530_0600','0600_0630','0630_0700','0700_0730','0730_0800','0800_0830','0830_0900',\n",
    "'0900_0930','0930_1000','1000_1030','1030_1100','1100_1130','1130_1200','1200_1230','1230_1300','1300_1330','1330_1400','1400_1430','1430_1500',\n",
    "'1500_1530','1530_1600','1600_1630','1630_1700','1700_1730','1730_1800','1800_1830','1830_1900','1900_1930','1930_2000','2000_2030','2030_2100',\n",
    "'2100_2130','2130_2200','2200_2230','2230_2300','2300_2330','2330_2400',\n",
    "'ad_id_history_exposure_times','ad_id_bid_mean','ad_id_bid_var','ad_id_pctr_mean','ad_id_pctr_var','ad_id_quality_ecpm_mean',\n",
    "'ad_id_quality_ecpm_var','ad_id_total_ecpm_mean','ad_id_total_ecpm_var', \n",
    "'account_id_history_exposure_times','account_id_bid_mean','account_id_bid_var','account_id_pctr_mean','account_id_pctr_var',\n",
    "'account_id_quality_ecpm_mean','account_id_quality_ecpm_var','account_id_total_ecpm_mean','account_id_total_ecpm_var',\n",
    "'commodity_id_history_exposure_times','commodity_id_bid_mean','commodity_id_bid_var','commodity_id_pctr_mean','commodity_id_pctr_var',\n",
    "'commodity_id_quality_ecpm_mean','commodity_id_quality_ecpm_var','commodity_id_total_ecpm_mean','commodity_id_total_ecpm_var',\n",
    "'commodity_type_history_exposure_times','commodity_type_bid_mean','commodity_type_bid_var','commodity_type_pctr_mean',\n",
    "'commodity_type_pctr_var','commodity_type_quality_ecpm_mean','commodity_type_quality_ecpm_var','commodity_type_total_ecpm_mean',\n",
    "'commodity_type_total_ecpm_var',\n",
    "'trades_id_history_exposure_times','trades_id_bid_mean','trades_id_bid_var','trades_id_pctr_mean','trades_id_pctr_var',\n",
    "'trades_id_quality_ecpm_mean','trades_id_quality_ecpm_var','trades_id_total_ecpm_mean','trades_id_total_ecpm_var',\n",
    "'week_day','have_create_seconds']\n",
    "# train_set.sort_values(by=['ad_id','bid'],inplace=True)\n",
    "label=train_set['exposure_times']\n",
    "train=train_set[columns_list]\n",
    "test=ad_sample_test[columns_list]\n",
    "label_validtion=valid_set_from_3_19['exposure_times']\n",
    "validtion=valid_set_from_3_19[columns_list]\n",
    "\n",
    "label_valid_array=label_validtion.values\n",
    "validtion_array=validtion.values\n",
    "train_array=train.values\n",
    "label_array=label.values\n",
    "test_array=test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyier/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtrain = xgb.DMatrix('agaricus.txt.train')\n",
    "def smape_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    biger_lesser_equation=(preds-labels)/(abs(preds-labels)+1e-16)\n",
    "    grad = biger_lesser_equation*(4*labels/(preds+labels)**2)\n",
    "    hess = (-1*biger_lesser_equation)*(8*labels/(preds+labels)**3)\n",
    "    return grad, hess\n",
    "def smape_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    temp_differ_abs=abs(preds-labels)\n",
    "    temp_sum_mod_2=(preds+labels)/2\n",
    "    return 'smape',sum(temp_differ_abs/temp_sum_mod_2)/labels.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(valid_,label_valid_array):\n",
    "    temp_differ_abs=abs(valid_-label_valid_array)\n",
    "    temp_sum_mod_2=(valid_+label_valid_array)/2\n",
    "    \n",
    "    print(40*(1-(sum(temp_differ_abs/temp_sum_mod_2)/label_valid_array.shape[0]/2)))\n",
    "    return sum(temp_differ_abs/temp_sum_mod_2)/label_valid_array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[0]\ttrain-rmse:90.6108\tvalid_data-rmse:85.4297\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:66.7638\tvalid_data-rmse:67.149\n",
      "[200]\ttrain-rmse:50.204\tvalid_data-rmse:56.8198\n",
      "[300]\ttrain-rmse:38.7327\tvalid_data-rmse:51.0865\n",
      "[400]\ttrain-rmse:30.8623\tvalid_data-rmse:48.6305\n",
      "[500]\ttrain-rmse:25.0543\tvalid_data-rmse:47.528\n",
      "[600]\ttrain-rmse:21.0434\tvalid_data-rmse:47.24\n",
      "[700]\ttrain-rmse:17.9741\tvalid_data-rmse:47.2237\n",
      "[800]\ttrain-rmse:15.7918\tvalid_data-rmse:47.5079\n",
      "Stopping. Best iteration:\n",
      "[637]\ttrain-rmse:19.7727\tvalid_data-rmse:47.1769\n",
      "\n",
      "fold n°2\n",
      "[0]\ttrain-rmse:96.4981\tvalid_data-rmse:53.6201\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:70.3092\tvalid_data-rmse:40.4439\n",
      "[200]\ttrain-rmse:52.6716\tvalid_data-rmse:34.7626\n",
      "[300]\ttrain-rmse:40.3382\tvalid_data-rmse:33.2924\n",
      "[400]\ttrain-rmse:31.6312\tvalid_data-rmse:33.8707\n",
      "[500]\ttrain-rmse:25.5193\tvalid_data-rmse:35.072\n",
      "Stopping. Best iteration:\n",
      "[311]\ttrain-rmse:39.1887\tvalid_data-rmse:33.2381\n",
      "\n",
      "fold n°3\n",
      "[0]\ttrain-rmse:91.1815\tvalid_data-rmse:83.3458\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:66.664\tvalid_data-rmse:66.4795\n",
      "[200]\ttrain-rmse:50.1444\tvalid_data-rmse:57.5908\n",
      "[300]\ttrain-rmse:38.9118\tvalid_data-rmse:53.0568\n",
      "[400]\ttrain-rmse:30.806\tvalid_data-rmse:50.8204\n",
      "[500]\ttrain-rmse:24.9617\tvalid_data-rmse:49.9653\n",
      "[600]\ttrain-rmse:20.8576\tvalid_data-rmse:49.6271\n",
      "[700]\ttrain-rmse:17.8263\tvalid_data-rmse:49.4791\n",
      "[800]\ttrain-rmse:15.6712\tvalid_data-rmse:49.6143\n",
      "Stopping. Best iteration:\n",
      "[654]\ttrain-rmse:19.0721\tvalid_data-rmse:49.4578\n",
      "\n",
      "fold n°4\n",
      "[0]\ttrain-rmse:90.6333\tvalid_data-rmse:85.5706\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:65.5981\tvalid_data-rmse:73.2676\n",
      "[200]\ttrain-rmse:48.8601\tvalid_data-rmse:70.1276\n",
      "[300]\ttrain-rmse:37.1574\tvalid_data-rmse:71.9406\n",
      "[400]\ttrain-rmse:29.0697\tvalid_data-rmse:74.7118\n",
      "Stopping. Best iteration:\n",
      "[200]\ttrain-rmse:48.8601\tvalid_data-rmse:70.1276\n",
      "\n",
      "fold n°5\n",
      "[0]\ttrain-rmse:78.1048\tvalid_data-rmse:125.668\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:56.9557\tvalid_data-rmse:112.467\n",
      "[200]\ttrain-rmse:42.7664\tvalid_data-rmse:104.982\n",
      "[300]\ttrain-rmse:33.1533\tvalid_data-rmse:101.129\n",
      "[400]\ttrain-rmse:26.4577\tvalid_data-rmse:98.8334\n",
      "[500]\ttrain-rmse:21.7897\tvalid_data-rmse:97.6264\n",
      "[600]\ttrain-rmse:18.5699\tvalid_data-rmse:97.1331\n",
      "[700]\ttrain-rmse:16.1725\tvalid_data-rmse:96.8689\n",
      "[800]\ttrain-rmse:14.4227\tvalid_data-rmse:96.8531\n",
      "[900]\ttrain-rmse:13.1326\tvalid_data-rmse:96.9838\n",
      "Stopping. Best iteration:\n",
      "[740]\ttrain-rmse:15.4137\tvalid_data-rmse:96.7552\n",
      "\n",
      "CV score: 4011.18265266\n",
      "9.789885164629894\n",
      "cv smape: 1.5105057417685053\n",
      "validation: 27975.471447260636\n",
      "9.155099383023554\n",
      "1.5422450308488223\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 10}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb = np.zeros(len(train_array))\n",
    "predictions_xgb = np.zeros(len(test_array))\n",
    "valid_xgb=np.zeros(len(label_valid_array))\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_array, label_array)):\n",
    "    print(\"fold n°{}\".format(fold_ + 1))\n",
    "    trn_data = xgb.DMatrix(train_array[trn_idx], label_array[trn_idx])\n",
    "    val_data = xgb.DMatrix(train_array[val_idx], label_array[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n",
    "                    verbose_eval=100, params=xgb_params)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(train_array[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(test_array), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    valid_xgb += clf.predict(xgb.DMatrix(validtion_array), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, label_array)))\n",
    "print(\"cv smape:\",smape(oof_xgb, label_array))\n",
    "print('validation:',mean_squared_error(valid_xgb, label_valid_array))\n",
    "print(smape(valid_xgb,label_valid_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission_csv generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=pd.read_csv(\"../Data/A_preliminary/testA/test_sample.dat\",header=None,sep='\\t',\n",
    "                        names=['sample_id','ad_id','create_time','ad_size','ad_trades_id','commodity_type','commodity_id','ad_account_id',\n",
    "                              'when_ad_put','audience_targeting','bid'])\n",
    "print(test_sample.head())\n",
    "submission=test_sample[['sample_id','ad_id','bid']]\n",
    "submission.insert(submission.shape[1], 'predict_exposure', predictions_xgb)\n",
    "# submission.insert(submission.shape[1], 'predict_exposure', predictions_lgb)\n",
    "\n",
    "submission['predict_exposure']=submission['predict_exposure'].apply(lambda x : round(x,4))\n",
    "ad_id_exposure_predic=submission.groupby(['ad_id'])['predict_exposure'].aggregate(['mean','max','min']).reset_index()\n",
    "ad_id_dict={}\n",
    "for ad_id,mean,min_,max_ in zip(ad_id_exposure_predic['ad_id'],ad_id_exposure_predic['mean'],ad_id_exposure_predic['min'],ad_id_exposure_predic['max']):\n",
    "#     ad_id_dict[ad_id]=mean-(max_-min_)/8\n",
    "    ad_id_dict[ad_id]=mean\n",
    "#     ad_id_dict[ad_id]=max_\n",
    "\n",
    "print('how many ad_id exposure less 1 times :')\n",
    "nums=0\n",
    "for _ in ad_id_dict.values():\n",
    "    if _ < 1:\n",
    "        nums+=1\n",
    "print(nums)\n",
    "\n",
    "print('Monotonic !!!')\n",
    "ad_id_explosure_df=pd.DataFrame({\"ad_id\":list(ad_id_dict.keys()),\"explosion_init\":list(ad_id_dict.values())})\n",
    "submission_csv=test_sample[['sample_id','ad_id','bid']][:]\n",
    "submission_csv=submission_csv.merge(ad_id_explosure_df,on='ad_id',how='outer')\n",
    "submission_csv['explosion']=submission_csv['explosion_init']+submission_csv['bid']/1000\n",
    "submission_csv['explosion']=submission_csv['explosion'].apply(lambda x:round(x,4))\n",
    "# submission_csv[['sample_id','explosion']].to_csv('submission.csv',header=None,index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv['explosion'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv[['sample_id','explosion']].to_csv('submission_3_03_15_98.csv',header=None,index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smapeobj(preds, labels):\n",
    "    biger_lesser_equation=(preds-labels)/(abs(preds-labels)+1e-16)\n",
    "    grad = biger_lesser_equation*(4*labels/(preds+labels)**2)\n",
    "    hess = (-1*biger_lesser_equation)*(8*labels/(preds+labels)**3)\n",
    "    return grad, hess\n",
    "def smape(preds,labels):\n",
    "    temp_differ_abs=abs(preds-labels)\n",
    "    temp_sum_mod_2=(preds+labels)/2\n",
    "    return temp_differ_abs/temp_sum_mod_2\n",
    "pred_array=np.array([0.1,0.3,0.6,0.7,0.8,1.5,1.6,1.8,1.9,2,2.1, 2.3, 2.6, 2.7, 2.8, 3.5, 3.6, 3.8, 3.9, 4])\n",
    "label_array=np.array([2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2])\n",
    "print(smapeobj(pred_array,label_array))\n",
    "print(smape(pred_array,label_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 120,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_lgb = np.zeros(train_array.shape[0])\n",
    "predictions_lgb = np.zeros(test_array.shape[0])\n",
    "valid_lgb=np.zeros(len(label_valid_array))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_array, label_array)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(train_array[trn_idx], label_array[trn_idx])\n",
    "    val_data = lgb.Dataset(train_array[val_idx], label_array[val_idx])\n",
    "\n",
    "    num_round = 10000*(fold_+1)\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 200)\n",
    "    oof_lgb[val_idx] = clf.predict(train_array[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test_array, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    valid_lgb += clf.predict(validtion_array, ntree_limit=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, label_array)))\n",
    "print(\"cv smape:\",smape(oof_lgb, label_array))\n",
    "print('validation:',mean_squared_error(valid_lgb, label_valid_array))\n",
    "print(smape(valid_lgb,label_valid_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=pd.read_csv(\"../Data/A_preliminary/testA/test_sample.dat\",header=None,sep='\\t',\n",
    "                        names=['sample_id','ad_id','create_time','ad_size','ad_trades_id','commodity_type','commodity_id','ad_account_id',\n",
    "                              'when_ad_put','audience_targeting','bid'])\n",
    "print(test_sample.head())\n",
    "submission=test_sample[['sample_id','ad_id','bid']]\n",
    "# submission.insert(submission.shape[1], 'predict_exposure', predictions_xgb)\n",
    "submission.insert(submission.shape[1], 'predict_exposure', predictions_lgb)\n",
    "\n",
    "submission['predict_exposure']=submission['predict_exposure'].apply(lambda x : round(x,4))\n",
    "ad_id_exposure_predic=submission.groupby(['ad_id'])['predict_exposure'].aggregate(['mean','max','min']).reset_index()\n",
    "ad_id_dict={}\n",
    "for ad_id,mean,min_,max_ in zip(ad_id_exposure_predic['ad_id'],ad_id_exposure_predic['mean'],ad_id_exposure_predic['min'],ad_id_exposure_predic['max']):\n",
    "#     ad_id_dict[ad_id]=mean-(max_-min_)/8\n",
    "    ad_id_dict[ad_id]=mean\n",
    "#     ad_id_dict[ad_id]=max_\n",
    "\n",
    "\n",
    "print('how many ad_id exposure less 1 times :')\n",
    "nums=0\n",
    "for _ in ad_id_dict.values():\n",
    "    if _ < 1:\n",
    "        nums+=1\n",
    "print(nums)\n",
    "\n",
    "print('Monotonic !!!')\n",
    "ad_id_explosure_df=pd.DataFrame({\"ad_id\":list(ad_id_dict.keys()),\"explosion_init\":list(ad_id_dict.values())})\n",
    "submission_csv=test_sample[['sample_id','ad_id','bid']][:]\n",
    "submission_csv=submission_csv.merge(ad_id_explosure_df,on='ad_id',how='outer')\n",
    "submission_csv['explosion']=submission_csv['explosion_init']+submission_csv['bid']/1000\n",
    "submission_csv['explosion']=submission_csv['explosion'].apply(lambda x:round(x,4))\n",
    "# submission_csv[['sample_id','explosion']].to_csv('submission.csv',header=None,index=None,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
